# ToolBox for LLM Research

This repo contains serveral initial code for three popular architectures, lncluding Sparse Mixture-of-Experts, Vision Transformer and Large Language Models. Sparse Mixture-of-Experts based on [FasterMoE](https://github.com/thu-pacman/FasterMoE); Vision Transformer based on [Deit](https://github.com/facebookresearch/deit), and Large Language [Huggingface Face](https://github.com/huggingface/transformers). Please refer to the original repo for the requirements. Some examples are provided in each folder.



